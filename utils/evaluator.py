import numpy as np
from collections import defaultdict
from config import EVAL_CONFIG

class Evaluator:
    """
    calculate evaluation metrics.
    """
    def __init__(self):
        self.k_values = EVAL_CONFIG['k_values']

    def calculate_metrics(self, predictions, ground_truths):
        """
        Calculates and returns a series of recommendation metrics.

        Args:
            predictions (list of lists): A list of recommended item IDs generated by the model for each user.
                                          e.g., [[item1, item2], [item3, item4], ...]
            ground_truths (list): The actual next item ID (ground truth) that each user interacted with.
                                  e.g., [gt1, gt2, ...]
        
        Returns:
            dict: A dictionary containing all the metrics, e.g., {'Recall@5': 0.1, 'NDCG@10': 0.05}
        """
        metrics = defaultdict(list)
        
        for k in self.k_values:
            recalls = []
            ndcgs = []
            
            for pred_list, gt_item in zip(predictions, ground_truths):
                # get top-k recommendations
                top_k_preds = pred_list[:k]
                
                # --- Calculate Recall@k ---
                recall = 1.0 if gt_item in top_k_preds else 0.0
                recalls.append(recall)
                
                # --- Calculate NDCG@k ---
                if gt_item in top_k_preds:
   
                    rank = top_k_preds.index(gt_item) + 1
                    idcg = 1.0
                    dcg = 1.0 / np.log2(rank + 1)
                    ndcg = dcg / idcg
                else:
                    ndcg = 0.0
                ndcgs.append(ndcg)
            
            metrics[f'Recall@{k}'] = np.mean(recalls)
            metrics[f'NDCG@{k}'] = np.mean(ndcgs)
            
        return dict(metrics)